{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../module/\")\n",
    "sys.path.append(\"../learning/\")\n",
    "\n",
    "import pickle\n",
    "import menconn\n",
    "from typelinking import *\n",
    "import time\n",
    "import marisa_trie\n",
    "import pickle\n",
    "from dataset import *\n",
    "from wikidata_linker_utils.offset_array import OffsetArray\n",
    "import train_type as tp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from functools import partial\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../ja_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "dataroot = '..'\n",
    "tagger = tp.SequenceTagger(os.path.join(dataroot,'ja_model/'))\n",
    "type_oracle = load_oracle_classification(os.path.join(dataroot, \"data/classifications/type_classification\"))\n",
    "trie_index2indices_values, trie_index2indices_counts, trie = load_trie(os.path.join(dataroot, 'data/ja_trie'))\n",
    "with open(os.path.join(dataroot, 'data/wikidata/indicies2wikititle.pkl'), 'rb') as hdl:\n",
    "    indices2title = pickle.load(hdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:0.001161336898803711\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "start = time.time()\n",
    "mtag = MeCab.Tagger('-Owakati')\n",
    "sentence = \"\"\"\n",
    "バラク・オバマは基本的に言ってインテリ層に人気のある黒人だったが、\n",
    "ドナルド・トランプは白人主義者や陰謀論者から人気を集めている。\n",
    "\"\"\"\n",
    "sentence_parsed = mtag.parse(sentence)\n",
    "ts = []\n",
    "sep = ''\n",
    "for n in range(1, 10):\n",
    "    n_grams = ngrams(sentence_parsed.split(), n)\n",
    "    for grams in n_grams:\n",
    "        ts.append(sep.join(list(grams)))\n",
    "ts = [t for t in ts if trie.get(t) is not None]\n",
    "end = time.time()\n",
    "print('Time:{}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:0.019841432571411133\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "results = None\n",
    "tokenize = partial(menconn.ja_tokenize, ts=ts)\n",
    "sent_splits, model_probs = solve_model_probs(sentence, tagger, tokenize=tokenize)\n",
    "entities = run(ts, sent_splits, model_probs, indices2title, type_oracle, trie, trie_index2indices_values, trie_index2indices_counts)\n",
    "preds = []\n",
    "for entity in entities:\n",
    "    if entity is not None:\n",
    "        preds.append(entity['ja'])\n",
    "    else:\n",
    "        preds.append(None)\n",
    "results = [{'mention':x, 'pred': y} for x,y in zip(ts,preds)]\n",
    "end = time.time()\n",
    "print(\"Time:{}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mention': 'バラク', 'pred': 'バラク (チャガタイ家)'},\n",
       " {'mention': '・', 'pred': '中黒'},\n",
       " {'mention': 'オバマ', 'pred': 'バラク・オバマ'},\n",
       " {'mention': 'は', 'pred': 'は'},\n",
       " {'mention': '基本', 'pred': '基本'},\n",
       " {'mention': '的', 'pred': '的'},\n",
       " {'mention': 'に', 'pred': '日本の鉄道駅一覧 に'},\n",
       " {'mention': 'て', 'pred': 'て'},\n",
       " {'mention': 'インテリ', 'pred': 'インテリ'},\n",
       " {'mention': '層', 'pred': '層 (数学)'},\n",
       " {'mention': 'に', 'pred': '日本の鉄道駅一覧 に'},\n",
       " {'mention': '人気', 'pred': '人気'},\n",
       " {'mention': 'の', 'pred': 'の'},\n",
       " {'mention': 'ある', 'pred': '存在記号'},\n",
       " {'mention': '黒人', 'pred': '黒人'},\n",
       " {'mention': 'た', 'pred': 'た'},\n",
       " {'mention': 'が', 'pred': 'が'},\n",
       " {'mention': '、', 'pred': '読点'},\n",
       " {'mention': 'ドナルド', 'pred': 'ドナルドダック'},\n",
       " {'mention': '・', 'pred': '中黒'},\n",
       " {'mention': 'トランプ', 'pred': 'トランプ'},\n",
       " {'mention': 'は', 'pred': 'は'},\n",
       " {'mention': '白人', 'pred': '白人'},\n",
       " {'mention': '主義', 'pred': '主義'},\n",
       " {'mention': '者', 'pred': '個人'},\n",
       " {'mention': 'や', 'pred': 'や'},\n",
       " {'mention': '陰謀', 'pred': '陰謀'},\n",
       " {'mention': '論', 'pred': '論'},\n",
       " {'mention': '者', 'pred': '個人'},\n",
       " {'mention': 'から', 'pred': '1月2日'},\n",
       " {'mention': '人気', 'pred': '人気'},\n",
       " {'mention': 'を', 'pred': 'を'},\n",
       " {'mention': '集め', 'pred': 'コレクション'},\n",
       " {'mention': 'て', 'pred': 'て'},\n",
       " {'mention': 'いる', 'pred': '入部'},\n",
       " {'mention': '。', 'pred': '句点'},\n",
       " {'mention': 'のある', 'pred': '熊野丸'},\n",
       " {'mention': 'だった', 'pred': 'フジネットワーク'},\n",
       " {'mention': 'たが', 'pred': '箍'},\n",
       " {'mention': '主義者', 'pred': '主義者'},\n",
       " {'mention': '陰謀論', 'pred': '陰謀論'},\n",
       " {'mention': 'バラク・オバマ', 'pred': 'バラク・オバマ'},\n",
       " {'mention': 'ドナルド・トランプ', 'pred': 'ドナルド・トランプ'},\n",
       " {'mention': '陰謀論者', 'pred': '陰謀論'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
